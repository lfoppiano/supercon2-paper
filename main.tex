\documentclass[a4paper]{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{authblk}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{array}

\title{Semi-automatic staging area for high-quality structured data extraction from scientific literature}
\author[1,2]{Luca Foppiano}
\author[1]{Tomoya Mato}
\author[3]{Kensei Terashima}
\author[4]{Pedro Ortiz Suarez}
\author[3]{Taku Tou}
\author[3]{Chikako Sakai}
\author[3]{Wei-Sheng Wang}
\author[2]{Toshiyuki Amagasa}
\author[3]{Yoshihiko Takano}
\author[1]{Masashi Ishii}
\affil[1]{Materials Modelling Group, Data-driven Materials Research Field, Centre for Basic Research on Materials, NIMS, Japan}
\affil[2]{Knowledge and Data Engineering, Centre for Computational Sciences, University of Tsukuba, Japan}
\affil[3]{Frontier Superconducting Materials Group, MANA, NIMS, Tsukuba, Japan}
\affil[4]{DFKI GmbH, Germany}

\begin{document}

\maketitle

\begin{abstract}
    TBA
\end{abstract}

\section{Introduction}

The emergence of new methodologies using machine learning for materials exploration has given rise to a growing research area called materials informatics (MI) ~\cite{10.3389/fchem.2022.930369}.
This field leverages the knowledge of the materials data accumulated in the past to efficiently screen candidates of the materials with desired properties.
As a matter of course, such an approach requires a larger number of material-related data for training models.
Researchers have been developing large aggregated databases of physical properties generated by first-principles calculations based on Density Functional Theory (DFT), such as Materials Project~\cite{materialsprojectJain2013}, JARVIS~\cite{aflowcurtarolo2012aflow}, NOMAD~\cite{nomad} etc., that played a role of a strong driving force for the development of materials informatics. 
Use of DFT data for machine learning in materials science has become popular since, in principle, it allows researchers to simulate and obtain various types of physical properties of the target materials, only by knowing the crystal structures of the subjects. 
Those DFT codes are designed so that they reproduce/simulate the physical properties that should be observed by experiment in reality. 
However, one needs to be careful when applying such calculated values to building machine learning model to guide experiments, as their predictions are not necessarily always valid under certain approximation of interactions between atoms and electrons in solids such as electron-electron Coulomb correlation, spin-orbit coupling and so on.

Au contrair, accumulated datasets of experimental data from scientific publications are still scarse, despite abundant availability of publications, and exponential growth in materials science~\cite{Pratheepan_2019}.
Currently, only a few limited resources, such as the Pauling File~\cite{Blokhin2018ThePF_paulingFile} and SuperCon~\cite{SuperCon}, exist, necessitating the reliance on manual extraction methods. 
This scarcity can be attributed to inadequate infrastructure and a shortage of expertise in the field.

% The lack of adequate infrastructure and expertise could have led to the creation of a single manual procedure that can extract information from diverse sources like plots, tables, and text all at once. However, while this approach may be viable in the short term, its sustainability diminishes over time. 
% On the other hand, constructing an automated process to accomplish this task presents many challenges. In the case of scientific publications, plots, tables, and text necessitates different treatments, and the resulting outputs must be merged and verified manually. Despite the challenges, it is possible to transition gradually towards automation by implementing iterative steps. This iterative approach involves reducing human involvement progressively while simultaneously optimising the efficiency of required human actions. 

SuperCon~\cite{SuperCon} was built manually from 1987~\cite{ishii2023structuring} by the National Institute for Materials Science (NIMS) in Japan and it is considered the gold standard in superconductors research.
Despite being praised for its excellent quality in numerous reports~\cite{roter2020predicting, stanev_machine_2017, tran2022machine, konno2021deep}, the updates of SuperCon have become increasingly challenging due to the high publication rate. However, in response to the need for a more efficient approach to sustain productivity, we embarked on the development of an automated system for extracting material and property information from the text contained in relevant scientific publications~\cite{lfoppiano2023automatic}. This automated process enabled the rapid creation of SuperCon\textsuperscript{2}, a comprehensive database of superconductors containing over 35,000 entries, within an operational duration of just a few days. 

Ensuring the same quality as SuperCon while automating the extraction of structured superconductors data poses significant challenges. 
We developed a web interface designed to facilitate the curation process, involving the active and ongoing management of data through its lifecycle of interest, specifically tailored for our superconductors database but open to the potential adaptation of other data structures. 
Our interface aims to maintain quality, add value, and provide for reuse over time while making the curation process more effective, and user-friendly.

There are several tools for data annotation, such as Inception~\cite{klie-etal-2018-inception}, and Doccano~\cite{doccano}, at the moment of writing this article, we are not aware of any other curation tools for materials extracted databases. 
This paper introduces our curation tool, SuperCon\textsuperscript{2}, which serves as a data staging area for SuperCon. 
The tool allows for the visualisation, correction, and integration of automatically extracted data into the SuperCon database while triggering a feedback loop to improve the machine learning (ML) models.

Our contributions to the field can be summarised as follows:
\begin{itemize}
    \item A new interface to curate and validate automatically extracted materials databases. We measured and reported an improved accuracy as compared with the manual method.
    \item a scalable ingestion process for creating a persistent database of superconductors materials and their related properties.
    \item an integrated simple anomaly detection process for the identification of outliers in the materials-properties database, 
    \item an enhanced PDF document viewer~\cite{wang2022hammer} applied to materials science, showing the materials entities from the extracted database. 
    \item the mechanism that generates training data based on curation which selects more relevant data for improving the ML models.
\end{itemize}

The subsequent section (Section~\ref{sec:ingestion}) discusses the ingestion process and the data model, then in Section~\ref{sec:user-interface} we illustrate the interface, Section~\ref{sec:data-correction} describes the data correction for automatically extracted entities, anomaly detection and the interface evaluation. 


\section{Ingestion process}
\label{sec:ingestion}

The ingestion process (Figure~\ref{fig:map-reduce}) is designed using a Map-Reduce approach. 
We briefly introduced it in our previous work~\cite{lfoppiano2023automatic} and we discuss the technical details in this section. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/ingestion-schema.png} 
  \caption{Ingestion process}
  \label{fig:map-reduce}
\end{figure}


The "Extraction Task" (Map) takes the PDF documents as input, saves them, and then processes them with Grobid-superconductors. 
Grobid-superconductors transforms the PDF documents into a rich representation document including annotations and layout tokens as JSON files, which we will refer to as \textit{structured documents}.  
The "Aggregation Task" (Reduce) takes in input the structured document and reduces it to a table format where each row (we refer to as a \textit{summarised record}, or, simply \textit{record}) contains one material-tc-properties triplet.

\subsection{Architecture}
\label{sec:architecture}

The two tasks are implemented by two Python scripts that can run asynchronously and with configurable scalability.
The storage is implemented using MongoDB~\footnote{\url{https://www.mongodb.com}}, an open-source document database. 
Overall, this process uses four main collections to store information: 
\begin{itemize}
    \item the \textbf{binary} collection contains the original pdf documents 
    \item the \textbf{documents} collection contains the structured document
    \item \textbf{tabular} collection stores the summarised records, and finally 
    \item \textbf{logger} contains detailed information on the processing status of each document. More details in Section~\ref{subsec:curation-and-processing-logs}.
\end{itemize}
There are also other collections used by the interface which are discussed in the related section (Section~\ref{subsec:feedback-loop-training-data}).

We compute the unique signature for each original document by using the first 10 characters of the MD5 hash function on the binary content. 
We use this information to link the original document, the structured document and the summarised records.


\subsection{Data formats}

In the "Extraction Task", Grobid-superconductors takes a PDF document as input, extracts entities, links, and returns the structured document as JSON file containing: a) bibliographic data, b) runtime execution, and c) a list of text passages (sentences or paragraphs) (Figure~\ref{fig:data-flow-2}). 

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/data-flow-2} 
  \caption{Example of the information from one single entity from a passage extracted in the "Extraction Task". The different structured information are highlighted: links, attributes, PDF boxes coordinates (to visualise annotations on the PDF document), and annotations references within the text passage (to visualise annotations on text).}
  \label{fig:data-flow-2}
\end{figure}

Each passage is composed of the following attributes: the text of the passage, the type of passage (whether it's a sentence or paragraph), the main section: header, body, and annex, and the subsections: title, abstract, paragraph, caption.
Furthermore, a passage contains also the list of spans representing the extracted entities and a list of layout tokens. 
The spans are characterised by text, type, attributes, a unique identifier, and other internal information (e.g. from which ML model the entity was extracted). 
The attributes are stored as a key-value and mainly contain information extracted by the material parser such as for example, the chemical formula, the structured composition, the material class, and so on.
The layout tokens contain low-level information coming from the PDF document: font size, font face, superscript, subscript, bold, italic, and the coordinates within the PDF document. The coordinates are expressed as couple of points (x, y) representing the vertixes of the "boxes" which encapsulate groups of token belonging to each annotation (Figure~\ref{fig:pdf-view}).  

The "Aggregation Task" is also processed by grobid-superconductors, using a different REST API URL. Here the input is the structured document and the output is a list of rows and columns of summarised records (Figure~\ref{fig:data-flow-3}). 
The aggregation pivots around the relation materials-Tc and attaches additional elements to it. Since the Extraction Task tends to match large entities, and they might contain information on multiple materials, for example, multiple records are created. 
For example, when the substitution variables are within the material expression, they result in multiple materials being extracted. For example "M Fe O (M=La,Cu)" is extracted as "La Fe O" and "Cu Fe O". 
In many cases inferring the correct interpretation is left to curators. For example, the expression "Zn and Cu doping La Fe B" possibly has two meanings. Namely, it means LaFeB doped with both Zn and Cu at the same time, or LaFeB doped with Zn, and LaFeB doped with Cu. 
Each record span has its own unique identifier which is calculated using the span attributes such as text, offsets, type, etc. 

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{images/data-flow-3} 
  \caption{Example of record related to the FeSe material after the aggregation.}
  \label{fig:data-flow-3}
\end{figure}

\section{User interface}
\label{sec:user-interface}

The SuperCon\textsuperscript{2} curation interface offers several key features to facilitate the data curation process.
It provides a comprehensive view of materials and their related properties as a table which includes search, filtering, and sorting functionality (Figure~\ref{fig:curation-interface-database}). 
The schema consists of two main classes: material information (material names, formulas, shape, etc.), properties (T\textsubscript{c}), and conditions (applied pressure, measurement method, etc.). The complete list including examples is reported in our previous work~\cite{lfoppiano2023automatic}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{images/supercon-curation-database} 
  \caption{Curation interface showing the database as a table}
  \label{fig:curation-interface-database}
\end{figure}


During the curation process is often necessary to navigate back and forth between the extracted record that is being examined and the related context in the paper (the related paragraph or sentence). 
The curation interface provides a document viewer combining a table with the extracted records and the viewer of their respective document (Figure~\ref{fig:pdf-view}). The viewer highlights the annotations that identify materials and properties, enabling users to easily locate and reference the extracted information within the document.

Users can add, amend, remove, or mark the records that are presented to them.  
Marking a record refers to the act of applying a visual or symbolic indication to a specific record, which can be "invalid" or "validated". 
Adding new records is limited to documents already in the database.

The interface automatically collects training data, when a record is amended, the information pertaining it's raw source information (sentence text, annotations) are collected. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{images/pdf-view-context.png} 
  \caption{PDF document viewer showing an annotated document. The table on top is linked through the annotated entities. The user can navigate from the record to the exact point in the PDF, with a pointer (the red bulb light) identifying the context of the entities being examined. }
  \label{fig:pdf-view}
\end{figure}

% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=1\textwidth]{images/supercon-curation-pdf-viewer} 
%   \caption{PDF viewer. The page includes a table showcasing records extracted from the  current document, along with the PDF content and accompanying annotations.}
%   \label{fig:curation-interface-pdf-viewer}
% \end{figure}

\subsection{Design principles}
\label{subsec:design-principles}

The design principle is a set of rules we chose at the beginning of the development. We define "curation" the overall process of correcting and validating information, and "correction" the process of modifying the values of a single item. 

\paragraph{Blind curation}
This interface has been built without any multi-user mechanism. 
The data is allocated to different curators by generating randomised links to the application. Each link opens records extracted from a specific document. 
The justification for this approach is twofold: a) the allocation is totally random and each curation does not know any information about the others, and b) the multi-user implementation requires a large effort without any particular scientific gain. 

\paragraph{Data-driven architecture} When a record is amended, the corrected information are stored in a new entry linked to the original one. 
Similarly, when a record is deleted, only its status is modified as "removed", and the record is hidden. 
With this approach we preserve any information, allowing also to generate time series of modified curation records such as which field was corrected, when, and how many changes were done. 
Such information allows the implementation of undo/redo functionality without changes in the data model if needed in the future. 

On the contrary, to save disk space, we do not collect duplicated PDF documents. We use the content hash (Section~\ref{sec:architecture}) as a unique identifier.
Obviously, this does not avoid loading two different PDF documents of the same article, because the hash code would be different. In such cases, anomaly detection (Section~\ref{subsec:anomaly-detection} will detect the duplication by checking the duplication by bibliographic data. 


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{images/record-correction} 
  \caption{Schema of the curation workflow. Each state is characterised by two properties: type and status, and one action, as indicated in the top right corner. "Error type" indicates the action of storing the error type for that specific action.}
  \label{fig:curation-workflow}
\end{figure}

\paragraph{Type and Status} Each \textit{record} has two workflow-related fields which are combined to identify the state of the workflow, which is illustrated in Figure~\ref{fig:curation-workflow}. 
The status indicates the record curation status from the data point of view (e.g. validated, invalid, curated, etc.), and is summarised in Table~\ref{tab:record-status}

\begin{table}[htbp]
\centering
\begin{tabular}{lp{10cm}}
\toprule
\textbf{Status} & \textbf{Description} \\
\midrule
new & Default status when a new record is created. \\
curated & The record has been amended manually. \\
validated & The record was validated manually. \\
invalid & The record is wrong or inappropriate for the situation (e.g., T\textsubscript{m} or T\textsubscript{curie} extracted as superconducting critical temperature). \\
obsolete & Assigned to a record when it is modified, triggering the creation of a new record (internal status, not visible to users).\\
deleted & The record has been removed by a curator (internal status, not visible to users). \\
\bottomrule
\end{tabular}
\caption{Record status definitions}
\label{tab:record-status}
\end{table}
    
The type indicates whether the record has been created or modified, manually or automatically. 
For example, the value "automatic" is provided when the data is loaded or when the anomaly detection performs some operations. In all other cases, it is flipped to "manual". 

\paragraph{Error types} Error types were first introduced in~\cite{lfoppiano2023automatic} while performing manually the end-to-end evaluation. They were combined with the evaluation to provide more detailed information on the reasons why certain extracted values were not correct. 
Since such statistics demonstrated to be useful during the development, we have extended the scope to add additional values related to data curation and validation (Table~\ref{tab:error-types}).

\begin{table}[htbp]
\centering
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Name} & \textbf{Description} \\
\midrule
From table & The entities Material $\rightarrow$ Tc $\rightarrow$ Pressure are identified in a table. At the moment, table extraction is not performed. \\
Extraction & The material, temperature, and pressure are not extracted (no box) or extracted incorrectly. \\
Linking & The material is incorrectly linked to the Tc given that the entities are correctly recognized. \\
Tc classification & The temperature is not correctly classified as "superconductors critical temperature" (e.g., Curie temperature, Magnetic temperature...). \\
Composition resolution & The exact composition cannot be resolved (e.g., the stoichiometric values cannot be resolved). \\
Value resolution & The extracted formula contains variables that cannot be resolved, even after having read the paper. This includes when data is from tables. \\
Anomaly detection & The data is automatically modified by the anomaly detection script. \\
Curation amends & The curator is updating the data which does not present issues due to the automatic system. \\
\bottomrule
\end{tabular}
\caption{Summary of error type values and description.}
\label{tab:error-types}
\end{table}

In the curation interface, we made the error type mandatory anytime the record is amended or removed. 
Since often the modifications are not related to a mistake, e.g. adding a space in a formula to make it more clear, or replacing garbled characters, we allow an additional error type called "Curation amend" which indicates that the problem is not related to the automatic system. This covers also the case where the modification corrects a mistake from another curator. 
When the update is made, a new record is created, and the error type is stored in the previous record. The workflow schema in Figure~\ref{fig:curation-workflow} indicates "error type" in the states that require its selection. 

\subsection{Usability}
The criteria for evaluating the usability of a user interface can vary depending on the context and specific goals of the evaluation. 
In regard to SuperCon\textsuperscript{2}, the simplicity makes it easier to evaluate, considering the main requirement being a smooth transition between the database records and the context in the document. 
We consider the WEBUSE (WEBsite USability Evaluation Tool)~\cite{chiew2003webuse} evaluation criteria for our analysis. WEBUSE has been considered for evaluating web pages usability in many works and it can be summarised as follows: 
\begin{itemize}
    \item Content, organisation, and readability. The usability of a user interface can be evaluated based on how well the content is presented (clear and concise information) and organised (logical organisation), and how easily readable it is. 
    \item Navigation and links. The ease of navigation and the effectiveness of links within the user interface are important criteria for usability evaluation. 
    \item User interface design. The interface should be visually pleasing and visually consistent throughout the application or website. 
    \item Performance and effectiveness. Performance and efficiency in making the users achieve their goals.
\end{itemize}

We discuss the four criteria as follows. 
\paragraph{Content, organisation and readability} The interface is simple and organised in two main pages: database and document. Since the database contains many columns we have reduced the ones visible by default, to make the table fit on the average screen. We use colours to separate records belonging to different documents. 

\paragraph{Navigation and links} The interface was designed with a minimalist approach. 
There are links and shortcuts in many duplicated positions to reduce the effort for certain repeated actions.
Finally, we provide an alternative and more efficient approach to navigation and correction using only the keyboard. 

\paragraph{User interface design}
The user interface is designed on top of the Vue.js JavaScript framework~\cite{vuejs}.  
Vue.js enables reactive operations: all changes in the page (e.g. text input and filtering) are applied to the web page without reloading. 
Additionally, it adopts the Vuetify design framework\cite{vuetify}, which adheres to Google's recommended Material Design principles.
These principles promote a unified and responsive user interface that incorporates elements inspired by physical materials ultimately aiming to create a delightful and engaging user experience.

\paragraph{Performance and effectiveness}
We designed the interface to reduce the actions required by users to transition from the record to the contextual information. 
The context is accessible in two ways: in the database view (Figure~\ref{fig:curation-interface-database}) we visualise the sentence in which the record belongs decorated with colours for each annotation relevant to the record.
The document view provides both the PDF document decorated with the extracted entities and the table of the records extracted from the document. The users can jump from the table to the related part of the document (Figure~\ref{fig:pdf-view}). 
Both the decoration and the links between records and document areas reduce the time needed to search for information in the document. 

\subsection{Curation and processing logs}
\label{subsec:curation-and-processing-logs}

The Supercon\textsuperscript{2} interface give access to information regarding the ingestion (processing log) and the curation process (curation log). 
The processing log is filled-up when the data is ingested, it was build to have minimal functions able to explain why certain documents haven't been processed (Figure~\ref{fig:processing-curation-log}). 
For example, sometimes documents are failing because they don't contain any text (image PDF documents) or they are too big (more than 100 pages). 
% Grobid was built focusing on speed and robustness, and contains several fail-safe mechanisms to avoid crashing the system when a document is either too big or does not contains valuable information, for example does not have any text. 
% Old PDF documents (e.g. before 1990) are likely have been scanned and contains only images. 
% Examples of too big documents are dissertation thesis with more than 100 pages, that might be collected by mistake. 


\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{images/processing-curation-log.png} 
  \caption{On the top: Processing log, showing the output of each operation (process document, create a record) and the outcome with the exception or error that should have occurred. On the bottom: Curation log, indicating each record, the number of updates, and the date/time of the last updates.}
  \label{fig:processing-curation-log}
\end{figure}

The curation log provides a view of what, when and how a record has been corrected (Figure~\ref{fig:processing-curation-log}).

% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=1\textwidth]{images/curation-log} 
%   \caption{Curation log, indicating each record, the number of updates, and the date/time of the last updates. }
%   \label{fig:curation-log}
% \end{figure}


\section{Data correction for automatically extracted entities in SuperCon\textsuperscript{2}}
\label{sec:data-correction}

In this section, we describe the processes related to data correction using the user interface. 
First, (Section~\ref{subsec:anomaly-detection}) we discuss anomaly detection as the pre-processing phase. 
Then, the manual correction process is described in Section~\ref{subsec:manual_correction}. 
At each correction, we collect data to be fed back as training data to the ML model, which is covered in Section~\ref{subsec:feedback-loop-training-data}.

\subsection{Anomaly detection}
\label{subsec:anomaly-detection}
Anomaly detection is the process of identifying unusual events or patterns in data. 
In our context this means identifying data that are greatly different from the average information, or that are "impossible" or very unlikely.
This post-process was introduced in a limited scope to draw attention to certain cases during the curation.

The anomaly detection uses a rule-based approach and marks any record that match the following conditions:
\begin{itemize}
    \item the extracted T\textsubscript{c} is greater than room temperature (273 K), negative, or contains invalid characters and cannot be parsed (e.g. "41]")
    \item the chemical formula cannot be processed by an ensemble composition parser that combine Pymatgen~\cite{Ong2013}, and text2chem~\cite{kononova_text-mined_2019} 
    \item the extracted applied pressure cannot be parsed or falls outside the range 0 - 250 GPa.
\end{itemize}

The record are marked by setting the "status" to "invalid" and add the error type as "anomaly detection", so that are easy to identify.
Since this process may find false positives, its output requires validation from curators. 
For example, in certain contexts, T\textsubscript{c} values above room temperature or applied pressure up to 500 GPa may be valid in researchers' hypotheses, calculations, or simulated predictions. 

We tested the effectiveness of the anomaly detection on a small subset of the database containing 667 records. 
The detection found 17 anomalies in T\textsubscript{c}, 1 anomaly in applied pressure, and 16 anomalies in the chemical formulas. 
The percentage of anomaly detection results rejected by curators after rechecking was 23\% for Tc, 37\% in chemical formulas and 0\% for applied pressure. 
This indicate an appropriate effectiveness and a relative low rate of false positives, although a detailed study might be needed due to the small sample size.

When we ran the anomaly detection on the full SuperCon\textsuperscript{2} database, it identified 1506 records with invalid T\textsubscript{c}, 5021 records with chemical formula that was not parseable, and 304 records with invalid applied pressure.  
We also identified only 1440 materials that have been linked to multiple T\textsubscript{c} values. Further analysis and cross-references with this information may be added in future development. 

\subsection{Manual correction}
\label{subsec:manual_correction}
The manual correction is still indispensable for developing high-quality structured data since the automatically extracted data is not perfect as discussed in \cite{lfoppiano2023automatic}, and the anomaly detection (Section~\ref{subsec:anomaly-detection}) only detects "potential" problems. 
We employ only domain experts as curators in order to certify the correctness of the data outcome. Even among experts, experience plays an important role (Section~\ref{sec:interface-evaluation})
"Manual correction" includes the amendment or the removal of invalid data, and the addition of data that are present in the original document, but the system did not extract. 
To avoid worker dependence and ensure robustness in the process, we took two approaches. 
First, we used a double-round approach where the data is initially corrected by one person, and validated in a second round, by a different person. 
Second, we have built documentation for the curation as a form of guidelines through an iterative loop of processes, as discussed in our previous work on the construction of the annotated dataset SuperMat~\cite{foppiano2021supermat}. 
The loop includes four steps: (i) collect rules, based on observation and reasoning, (ii) curation following those rules (iii), retrospective including analysis and discussions based on curators' feedback, and (iv) take decisions and update the guideline.

The guidelines consist mainly of two parts: the general principles and the correction rules with examples of solutions.
The guidelines are designed to provide general information applied to corrections and very basic explanations containing illustrations for a faster understanding (e.g. the meaning of the colours of the annotations). This would help new curators to catch up with the required level of curation precision quickly. 
There are two main components described in the correction rules: the record that is being corrected and its context. 
The context of a record can be obtained by examining the extracted annotated text or the PDF document area.

The correction rules are described based on the error type mentioned in Section~\ref{subsec:design-principles}, and in the guideline, the description of rules is accompanied by sheets that explain five points to the curators, as illustrated in Figure~\ref{fig:example-curation-sheet}:
\begin{itemize}
    \item \textbf{Sample input data}, a screenshot of SuperCon\textsuperscript{2} record in the interface
    \item \textbf{Context}, a screenshot of the related part of the document (either from the PDF or from plain text sentences) that contain the extracted data to be curated,
    \item \textbf{Motivation}, describes the issue with the examined extracted data, 
    \item \textbf{Action} to be taken, 
    \item \textbf{Expected output}, a screenshot of the expected SuperCon\textsuperscript{2} record, after correction
\end{itemize}

% An example of a sheet can be found in Figure~\ref{fig:example-curation-sheet}. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{images/example-sheet-curation.png} 
  \caption{Example of curation sheet. As discussed, it's written with simple language assuming the curator may not be familiar with the task. }
  \label{fig:example-curation-sheet}
\end{figure}


\subsection{Automatic training data generation}
\label{subsec:feedback-loop-training-data}
The curation process is a valuable endeavour demanding significant knowledge and human effort. 
It is crucial to maximise the use of this time for collecting as much information as possible.
For this reason, we integrated an automatic procedure for accumulating training examples in the curation process. 
In the event of an amend (update, removal) of a database record, this process retrieves the corresponding raw data: the text passage, the recognised entities (spans), and the layout tokens information. 
This information are sufficient to be exported as training examples, which can be examined and corrected, and feed back to the ML model. 

In details, the process perform the following actions:
\begin{itemize}
    \item The updated record is prepared and stored.
    \item The raw data originating the updated record is identified. First, the corresponding structured document is retrieved from the document collection using the document identifier (the hash). Then, the exact text passage in the structured document is located using an unique id assigned to each material in the database records.
    \item If the raw data has already been collected, it is skipped. This is the case when multiple records belonging to the same text passage are corrected.
    \item Otherwise, the raw information comprising the text string, the spans, the layout tokens are collected and saved in a separate collection.
    \item The data collected is then sufficient to generate workable instances in different output format and the related feature files.
\end{itemize}

We designed a specific page of the interface (Figure~\ref{fig:training-data-view}) to manage the collected data in which each row corresponds to a training example, which includes the decorated text showing the identified entities, the document identifier, and the status. 
The users can examine the data, delete it, or send it to the annotation tool to be corrected. 
Depending on which state the records are, the status can be: "new" when the data is added, "in progress" after the data is sent to the annotation tool, and "exported" when the corrected training data is downloaded. 
We integrated Label-studio~\cite{Label_Studio} for the correction of the collected. Label-studio is an open-source, python based, and modern interface supporting many different TDM tasks (NER, topic modelling, image recognition, etc.). 

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{images/training-data-viewer} 
  \caption{Training data view}
  \label{fig:training-data-view}
\end{figure}

Since the training data generated with this interface are related to manual corrections in the TDM process, intuitively, they should have a higher impact on improving the ML model than randomly selected training data of the same amount. 
We selected around 400 Supercon\textsuperscript{2} extracted records initially marked by the anomaly detection script, and we curated them following the process explained previously. 
Then, we collected the raw data that was aggregated by our curation and we obtained a set of 352 examples (these did not include the false positives identified by the anomaly detection).

We call the obtained dataset \emph{curation} to be distinguished from original SuperMat dataset which is referred as \emph{base}.

We performed different training/evaluation test on the fine-tuning of our best model based on SciBERT~\cite{Beltagy2019SciBERT}. We use the DeLFT (Deep Learning For Text)~\cite{DeLFT} library for training, evaluating, and managing the models for prediction.  A model can be trained with two different strategies: 
\begin{enumerate}
    \item \emph{``from scratch''}: when the model is initialised randomly. We denote this strategy with an \emph{(s)}.
    \item \emph{``incremental''}: when the initial model weights are taken from an already existing model. We denote this strategy with an \emph{(i)}.
\end{enumerate}
The latter can be seen as a way to ``continue'' the training from a specific checkpoint. We thus define three different training protocols: 
\begin{enumerate}
    \item Using the \emph{base} dataset and training from scratch (s).
    \item Using both the \emph{base} and \emph{curation} datasets and training from scratch (s).
    \item Using the \emph{base} dataset to train from scratch (s), and then continuing the training with the \emph{curation} dataset (i).
\end{enumerate}
We merge ``curation'' with the base dataset because the curation dataset is very small compared to ``base'', an we want to avoid catastrophic forgetting~\cite{overcoming-kirkpatrick-etal-2016} or overfitting.

The trained models are then tested using a fixed holdout dataset that we designed in our previous work~\cite{lfoppiano2023automatic} and the evaluation scores are shown in Table~\ref{tab:evaluation-curation-training}.

\begin{table}[ht]
\centering\small
\begin{tabular}{lrrr}
\toprule
& \textbf{base}(s) & \textbf{(base+curation)(s)} & \textbf{base(s)+curation(i)} \\ 
\midrule
Nb total examples & 16902 & 17254 & 16902(s), 17254 (i)\\ 
\midrule
\texttt{<class>}        & 70,22             & 72,30             & \textbf{72,63} \\ 
\texttt{<material>}     & 79,69             & 80,23             & \textbf{80,61} \\ 
\texttt{<me\_method>}   & 64,78             & 65,31             & \textbf{66,62} \\ 
\texttt{<pressure>}     & \textbf{46,96}    & 46,53             & 46,84 \\ 
\texttt{<tc>}           & 77,36             & 78,56             & \textbf{79,57} \\ 
\texttt{<tcValue>}      & 77,26             & \textbf{77,94}    & 77,84 \\ 
\midrule
\textbf{All (micro avg.)} & 75,86           & 76,66             & \textbf{77,36} \\ 
\midrule
\textbf{$\Delta$ avg. w/ baseline}& -       & +0,80             & \textbf{+1,50} \\ 
\bottomrule
\end{tabular}
\caption{F1-score from the evaluation of the fine-tuning training of SciBERT. The training is performed with three different approaches. 
The \emph{base} dataset is the original dataset described in~\cite{lfoppiano2023automatic}, the \emph{curation} dataset is automatically collected based on the database corrections by the interface and manually corrected. \textit{s} indicate "training from scratch", while \textit{i} indicate "incremental training". 
The evaluation is performed using the same holdout dataset from SuperMat. 
The results are averaged over 5 runs. }
\label{tab:evaluation-curation-training}
\end{table}


\begin{table}[ht]
\centering
\begin{tabular}{lrrr}
\toprule
                        & \textbf{base}     & \textbf{base+curation}    & \textbf{$\Delta$}  \\ 
\midrule
\texttt{<class>}        & 1646              & 1732                      &  86                \\
\texttt{<material>}     & 6943              & 7580                      &  637               \\
\texttt{<me\_method>}   & 1883              & 1934                      &  51                \\
\texttt{<pressure>}     & 274               & 361                       &  87                \\
\texttt{<tc>}           & 3741              & 4269                      &  528               \\
\texttt{<tcValue>}      & 1099              & 1556                      &  457               \\
\midrule
\textbf{Total}          & 15586             & 17432                     & 1846               \\ 
\bottomrule
\end{tabular}
\caption{Data support, corresponding to the number of entities of a specific type.}
\label{tab:training-support}
\end{table}

The result of this experiment is that with only 352 examples (2\% of the SuperMat dataset) and 1846 more entities (11\% of the entities from the SuperMat dataset) (Table~\ref{tab:training-support}. 
We obtained an increment of +1.50 in F1-score\footnote{In our previous work~\cite{lfoppiano2023automatic} we reported 77.03\% F1-score. There is slight a decrease in absolute scores between DeLFT 0.2.8 and DeLFT 0.3.0. The most probable cause could be the impact of using Hugging Face library which is suffering of quality issues in particular in relation with their tokenizers implementation \url{https://github.com/kermitt2/delft/issues/150}}.

Here, the incremental approach obtained nearly twice the improvement of the model trained from scratch with the extended ``base+curation'' dataset. 
There are several hypotheses for this result, the first hypothesis is that the training dataset is not big enough for the task at hand, therefore the model requires more training time. 
This issue could be verified by correcting all the available training data and repeating this experiment. 
Another hypothesis that our data distribution being rather skewed (c.f. Table \ref{tab:training-support}) favours an incremental approach as the deltas in the support of the ``curation'' dataset with respect to the ``base'' dataset,  somehow counters the class imbalance. 
Finally, might also be that the hypeparameter we chose for our model, in particular the learning rate and batch size could be still better tuned in order to close the gap between the second and the third approach.

\section{Interface evaluation}
\label{sec:interface-evaluation}
We conducted an experiment to compare the efficiency and correctness of data extraction using two methods: a) the SuperCon\textsuperscript{2} user \textit{interface} or b) the "traditional approach" of reading \textit{PDF} documents and filling up an Excel file. 
Our sample consisted of 15 papers, which were distributed among three curators — a senior researcher, a PhD student, and a master student. 
We distributed 10 papers to each curator, with an equal split between \textit{interface} and \textit{pdf}. The overlap between curators was of 5 papers using the opposite method (e.g. curator A receive paper 1 to be corrected with the \textit{interface}, curator B, that shares the same paper has to curate it by reading the \textit{pdf})
The curated content was then revised by a fourth person manually and the revision was used for calculating the evaluation scores. 

We evaluated the two methods based on two trade-off aspects: efficiency, by comparing the time required for curation, and accuracy, measured in terms of precision, recall, and F1-score.

The comparison of the time taken revealed no significant difference between the interface and the traditional method. Specifically, the total time was only 4 minutes longer with the interface (188 minutes compared to 184 minutes). The time difference did not demonstrate any consistent trend, suggesting the need for a larger dataset in future experiments.

When we examined the accuracy of the extracted data, we observed a 6\% improvement in precision and a substantial 111\% improvement in recall when using the interface (Table~\ref{tab:evaluation-interface-correction}). 

\begin{table}[ht]
\centering
\begin{tabular}{lrrr}
\toprule
                    & \textbf{P (\%)}   & \textbf{R (\%)}   & \textbf{F1 (\%)}  \\
    \midrule
    PDF document    & 87.83             & 45.60             & 52.66             \\
    Interface       & \textbf{93.37}    & \textbf{96.39}    & \textbf{93.28}    \\
    \bottomrule
\end{tabular}
\caption{Results of the comparison between the curation using the SuperCon 2 interface and the traditional method of reading the PDF document. }
\label{tab:evaluation-interface-correction}
\end{table}

The disparity in experience significantly influenced the accuracy of curation, particularly in terms of high-level skills. Senior researchers consistently achieved an average F1-Score approximately 20\% higher than other curators (see Table~\ref{tab:accuracy-by-experience}). Furthermore, we observed a modest improvement between master's students and PhD students. These findings indicate also that for large-scale projects, employing master students instead of PhD students may be a more cost-effective choice. Thus, using only a few senior researchers for the second round of validation (Section~\ref{subsec:manual_correction}).

\begin{table}[h]
\centering
\begin{tabular}{lrrr}
\toprule
\textbf{Experience} & \textbf{P (\%)}   & \textbf{R (\%)}   & \textbf{F1 (\%)}  \\
\midrule
Master student      & 90.03             & 66.10             & 66.40             \\
PhD student         & 83.33             & 65.69             & 69.45             \\
Senior researcher   & \textbf{98.45}    & \textbf{81.22}    & \textbf{83.08}    \\
\bottomrule
\end{tabular}
\caption{Accuracy measured by curator experience}
\label{tab:accuracy-by-experience}
\end{table}

Finally, the collected data suggest that all three curators had overall more corrected results by using the interface as illustrated in Table~\ref{tab:accuracy-by-experience-method}. 

\begin{table}[h]
\centering
\begin{tabular}{lcrrr}
\toprule
\textbf{Experience} & \textbf{Method} & \textbf{P (\%)} & \textbf{R (\%)} & 
\textbf{F1 (\%)} \\
\midrule
\multirow{2}{*}{Master student} & PDF Document & 94.58 & 36.55 & 48.67 \\
 & Interface & 83.19 & 95.83 & 88.25 \\
\midrule
\multirow{2}{*}{PhD student} & PDF Document & 70.00 & 48.51 & 50.78 \\
 & Interface & 96.67 & 82.86 & 88.11 \\
\midrule
\multirow{2}{*}{Senior researcher} & PDF Document & \textbf{100.00} & 55.56 & 61.03 \\
 & Interface & 97.42 & \textbf{98.33} & \textbf{97.78} \\
\bottomrule
\end{tabular}
\caption{Accuracy measured by experience and method, PDF document vs Interface}
\label{tab:accuracy-by-experience-method}
\end{table}


The results of this experiment confirmed that utilising the interface in conjunction with an automated system required a comparable amount of time for curating SuperCon data compared to the "traditional method." However, it significantly improved the quality of the extracted data.
Additionally, several observations were made during the curation process:

\begin{itemize}
    \item The interface require a finite adaptation time, in particular at the beginning of the work. The curators that were starting the evaluation from the interface tented to tended to ask questions about the usage, primarily due to their lack of familiarity. 
    \item The interface demonstrated a substantial increase in recall. Our intuition suggests the interface overcome the tendency to overlook information when reading the plain PDF document.
\end{itemize}


\section{Code availability}
This application is freely available at \url{https://github.com/lfoppiano/supercon2}, the repository contains:
\begin{itemize}
\item the code of the SuperCon 2 curation interface for visualising and editing material and properties extracted from superconductors-related papers.
\item The ingestion workflow to create process PDF documents with Grobid-superconductors and produce a database of materials and properties.
\item the guidelines, accessible at \url{https://supercon2.readthedocs.io}
\end{itemize}

\section{Acknowledgements}
Our warmest thanks to Patrice Lopez, the author of Grobid~\cite{GROBID}, DeLFT~\cite{DeLFT}, and other open-source projects for his continuous support and inspiration with ideas, suggestions, and fruitful discussions.
We thank Pedro Baptista de Castro for his support during this work. 

\section{Conclusions}
We built a staging area for SuperCon that allow the feeding with high-quality data from TDM of materials and properties through efficient manual curation. 
The data is loaded through an ingestion process that loads a persistent database of materials and relative properties extracted automatically by Grobid-superconductors~\cite{lfoppiano2023automatic}. 
The data is then pre-processed with a simple anomaly detection rules to identify outliers and mark them.
We then developed a user interface for curating the new superconductors data extracted automatically, before it is sent to the SuperCon database. 
The interface combines the best practices in user interaction design and provides among many features: an enhanced PDF document visualisation and rapid transitions from the database records the related section in the original document. 
We reported that our interface achieves higher precision while requiring the same time for curating, as compared with the traditional manual process.
The interface also automatically collects data related to the correction that can be used to feed back the ML models as training data. 
We demonstrated that the feedback loop based on corrected data can substantially improve the machine learning models with fresh and targeted training data. 

There are several planned features and improvements in the pipeline. Some of these include:

\begin{itemize}
    \item Undo/redo functionality: The ability to undo and redo changes made to records will be added, to make it easier to correct mistakes.
    \item Document versioning: A versioning system will be implemented to track changes to documents over time.
    \item Improved search: The search functionality will be improved to make it easier to find records based on specific criteria.
    \item Additional record types: SuperCon\textsuperscript{2} currently supports records for material and property information, but additional record types will be added.
\end{itemize}

\section*{Contributions}
LF wrote the paper and everybody else revised it. 
LF and POS discussed the ML results and experiments. 
LF wrote the back end, and TM wrote the front end of the user interface. 
LF designed the user interface experiment with KT, TT and WS as curators.
KT lead the materials-science work on the data with CS, TT and WS.
TA revised the paper from CS's perspective.
YT and MI supervised the work. 


\bibliography{references}
\bibliographystyle{unsrt}

\end{document}



